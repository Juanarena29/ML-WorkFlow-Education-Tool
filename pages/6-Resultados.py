import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st

from src.ml.evaluator import (
    build_metrics_table,
    compute_confusion_matrix,
    compute_confusion_matrix_normalized,
    compute_residuals,
    compute_roc_curve,
    extract_feature_importance,
    get_train_test_split,
)
from src.utils.file_handler import save_model, save_project_config

from src.utils.session import check_step_access, get_project, initialize_session


def _render_metrics(metrics: dict, problem_type: str) -> None:
    if not metrics:
        st.info("No hay metricas disponibles.")
        return

    df_metrics = build_metrics_table(metrics)
    st.subheader("Tabla comparativa")
    st.dataframe(df_metrics, use_container_width=True)

    if problem_type == "regression":
        score_map = {
            "MAE ‚Äì Error absoluto medio": "mae",
            "RMSE ‚Äì Error cuadr√°tico medio": "rmse",
            "R¬≤ ‚Äì Capacidad explicativa": "r2",
        }
    else:
        score_map = {
            "Accuracy ‚Äì Exactitud": "accuracy",
            "Precision ‚Äì Confiabilidad de positivos": "precision",
            "Recall ‚Äì Cobertura de positivos": "recall",
            "F1 Score ‚Äì Balance Precision/Recall": "f1",
            "ROC AUC ‚Äì Probabilidades": "roc_auc",
        }

    available_score_map = {
        label: col for label, col in score_map.items() if col in df_metrics.columns}

    if not available_score_map:
        return

    metric_label = st.selectbox(
        "Metrica para comparar",   options=list(available_score_map.keys()))
    metric_name = available_score_map[metric_label]
    fig = px.bar(
        df_metrics.sort_values(metric_name, ascending=False),
        x=metric_name,
        y="modelo",
        orientation="h",
        title=f"Comparacion por {metric_label}",
    )
    st.plotly_chart(fig, use_container_width=True)


def _render_model_details(models: dict, metrics: dict, project, learn) -> None:
    st.subheader("Detalles por modelo")
    if learn:
        with st.expander("üìä ¬øQu√© es el detalle por modelo?"):
            st.markdown(
                "Este apartado muestra los **valores exactos de las m√©tricas** para cada modelo.\n\n"
                "A diferencia de los gr√°ficos, ac√° pod√©s comparar modelos de forma directa y objetiva.\n\n"
                "Usalo para confirmar cu√°l modelo rinde mejor seg√∫n la m√©trica que elegiste."
            )
    for model_name, model in models.items():
        with st.expander(model_name):
            st.write("Metricas")
            model_metrics = metrics.get(model_name, {})
            st.json(model_metrics)

            if project.best_params.get(model_name):
                st.write("Mejores hiperparametros")
                st.json(project.best_params[model_name])

            importances = extract_feature_importance(
                model,
                project.get_numeric_features(),
                project.get_categorical_features(),
            )
            if importances is None or importances.empty:
                st.write("Este modelo no expone importancias.")
            else:
                st.write("Feature importance (top 20)")
                if learn:
                    with st.expander("üîç ¬øQu√© significa la importancia de variables?"):
                        st.markdown(
                            "La **importancia de variables** indica qu√© columnas influyen m√°s en las predicciones del modelo.\n\n"
                            "Variables m√°s importantes tienen mayor impacto en el resultado final.\n\n"
                            "Esto ayuda a entender el modelo y a detectar qu√© datos son m√°s relevantes."
                        )
                top = importances.head(20)
                fig = px.bar(
                    top.sort_values("importance", ascending=True),
                    x="importance",
                    y="feature",
                    orientation="h",
                )
                st.plotly_chart(fig, use_container_width=True)


def _plot_confusion_matrix(y_true, y_pred) -> None:
    cm, labels = compute_confusion_matrix(y_true, y_pred)
    fig = px.imshow(
        cm,
        x=labels,
        y=labels,
        text_auto=True,
        labels={"x": "Predicho", "y": "Real"},
        title="Matriz de confusion",
    )
    st.plotly_chart(fig, use_container_width=True)


def _plot_confusion_matrix_normalized(y_true, y_pred) -> None:
    cm, labels = compute_confusion_matrix_normalized(y_true, y_pred)
    fig = px.imshow(
        cm,
        x=labels,
        y=labels,
        text_auto=".2f",
        labels={"x": "Predicho", "y": "Real"},
        title="Matriz de confusion (normalizada)",
    )
    st.plotly_chart(fig, use_container_width=True)


def _plot_residuals(y_true, y_pred) -> None:
    residuals = compute_residuals(y_true, y_pred)
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=y_pred, y=residuals, mode="markers"))
    fig.update_layout(
        title="Residuos vs Prediccion",
        xaxis_title="Prediccion",
        yaxis_title="Residuo",
    )
    st.plotly_chart(fig, use_container_width=True)


def _plot_roc_curve(y_true, pipeline, X_test) -> None:
    if y_true.nunique() != 2:
        return

    if hasattr(pipeline, "predict_proba"):
        y_score = pipeline.predict_proba(X_test)[:, 1]
    elif hasattr(pipeline, "decision_function"):
        y_score = pipeline.decision_function(X_test)
    else:
        return

    fpr, tpr, roc_auc = compute_roc_curve(y_true, y_score)
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode="lines",
                  name=f"AUC={roc_auc:.3f}"))
    fig.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1], mode="lines", name="Base", line=dict(dash="dash")))
    fig.update_layout(
        title="Curva ROC",
        xaxis_title="False Positive Rate",
        yaxis_title="True Positive Rate",
    )
    st.plotly_chart(fig, use_container_width=True)


def main() -> None:
    initialize_session()

    if not check_step_access("training"):
        return

    project = get_project()
    if not project.metrics:
        st.error("No hay resultados para mostrar.")
        return

    learn = project.ui_mode == "learn"

    st.title("Resultados y Comparaci√≥n")
    if learn:
        with st.expander("üìå ¬øQu√© estoy viendo en esta pantalla?"):
            st.markdown(
                "Ac√° pod√©s **comparar modelos** y entender cu√°l funciona mejor para tu dataset.\n\n"
                "Vas a ver:\n"
                "- Una comparaci√≥n por m√©tricas (n√∫meros).\n"
                "- Gr√°ficos por cada modelo (para ver aciertos/errores).\n"
                "- La opci√≥n de guardar el modelo que elijas."
            )
    _render_metrics(project.metrics, project.problem_type or "classification")
    if learn:
        with st.expander("üìè ¬øC√≥mo interpreto las m√©tricas?"):
            st.markdown(
                "Las m√©tricas son una forma de resumir qu√© tan bien predice el modelo.\n\n"
                "- En general, **m√°s alto es mejor** (por ejemplo: Accuracy, F1, R¬≤).\n"
                "- En m√©tricas de error, **m√°s bajo es mejor** (por ejemplo: MAE, RMSE).\n\n"
                "Lo importante es comparar modelos usando **la misma m√©trica**."
            )
    st.markdown("---")
    # Validaciones m√≠nimas (evita excepciones generales)
    if project.df_limpio is None or project.df_limpio.empty:
        st.error("No hay dataset limpio disponible para reconstruir el split.")
        _render_model_details(project.trained_models,
                              project.metrics, project, learn)
        return

    if not project.target_column:
        st.error("No se encontr√≥ la columna target del proyecto.")
        _render_model_details(project.trained_models,
                              project.metrics, project, learn)
        return

    if not project.train_test_split_config:
        st.error("No se encontr√≥ la configuraci√≥n de train/test split.")
        _render_model_details(project.trained_models,
                              project.metrics, project, learn)
        return

    if learn:
        with st.expander("üìä ¬øPara qu√© sirven los gr√°ficos por modelo?"):
            st.markdown(
                "Los gr√°ficos ayudan a ver el comportamiento real del modelo, no solo un n√∫mero.\n\n"
                "- En **clasificaci√≥n**, muestran qu√© clases se confunden entre s√≠.\n"
                "- En **regresi√≥n**, muestran qu√© tan lejos est√°n las predicciones de los valores reales.\n\n"
                "Si dos modelos tienen m√©tricas parecidas, los gr√°ficos suelen ayudarte a decidir mejor."
            )

    st.subheader("Gr√°ficos por modelo")

    if learn and project.problem_type == "classification":
        with st.expander("üß© ¬øQu√© es la matriz de confusi√≥n y la curva ROC?"):
            st.markdown(
                "**Matriz de confusi√≥n**: muestra aciertos y errores por clase.\n"
                "Ayuda a ver en qu√© casos el modelo se equivoca m√°s.\n\n"
                "**Curva ROC**: eval√∫a qu√© tan bien el modelo separa las clases usando probabilidades.\n"
                "Es √∫til cuando importa distinguir positivos y negativos."
            )

    # Reconstrucci√≥n del split con manejo de error m√°s espec√≠fico
    try:
        X_train, X_test, y_train, y_test = get_train_test_split(
            project.df_limpio,
            project.target_column,
            project.problem_type or "classification",
            project.train_test_split_config,
        )
    except (KeyError, ValueError, TypeError) as exc:
        st.error(f"No se pudo reconstruir el split: {exc}")
        _render_model_details(project.trained_models,
                              project.metrics, project, learn)
        return

    if not project.trained_models:
        st.warning("No hay modelos entrenados para graficar.")
        _render_model_details(project.trained_models,
                              project.metrics, project, learn)
        return

    for model_name, pipeline in project.trained_models.items():
        with st.expander(f"{model_name} - gr√°ficos"):
            # Predicci√≥n con fallback controlado
            try:
                y_pred = pipeline.predict(X_test)
            except (ValueError, TypeError) as exc:
                st.error(f"No se pudo predecir con {model_name}: {exc}")
                continue

            y_test_plot = y_test

            # Normalizaci√≥n/decodificaci√≥n solo para clasificaci√≥n, evitando excepciones generales
            if project.problem_type == "classification" and project.target_label_encoder is not None:
                y_pred_is_num = pd.api.types.is_numeric_dtype(
                    pd.Series(y_pred))
                y_test_is_num = pd.api.types.is_numeric_dtype(y_test_plot)

                # Si predicci√≥n es num√©rica pero y_test es texto/categor√≠a, intentar invertir predicci√≥n
                if y_pred_is_num and not y_test_is_num:
                    try:
                        y_pred = project.target_label_encoder.inverse_transform(
                            pd.Series(y_pred).astype(int)
                        )
                    except (ValueError, TypeError):
                        # Si no se puede invertir, intentamos transformar y_test a num√©rico para comparar
                        try:
                            y_test_plot = pd.Series(
                                project.target_label_encoder.transform(
                                    y_test_plot),
                                index=y_test_plot.index,
                            )
                        except (ValueError, TypeError):
                            # Si tampoco se puede, seguimos con lo que haya (sin romper)
                            pass

            if project.problem_type == "classification":
                if learn:
                    with st.expander("üß© ¬øQu√© es la matriz de confusi√≥n y la curva ROC?"):
                        st.markdown(
                            "**Matriz de confusi√≥n**: muestra aciertos y errores por clase.\n"
                            "Te ayuda a ver, por ejemplo, si el modelo confunde 'A' con 'B'.\n\n"
                            "**Curva ROC**: es una forma de evaluar modelos que trabajan con probabilidades.\n"
                            "Suele ser √∫til cuando quer√©s separar bien positivos y negativos."
                        )
                _plot_confusion_matrix(y_test_plot, y_pred)
                _plot_confusion_matrix_normalized(y_test_plot, y_pred)

                # ROC necesita y_test num√©rico si hay encoder
                y_test_roc = y_test_plot
                if project.target_label_encoder is not None and not pd.api.types.is_numeric_dtype(y_test_roc):
                    try:
                        y_test_roc = pd.Series(
                            project.target_label_encoder.transform(y_test_roc),
                            index=y_test_roc.index,
                        )
                    except (ValueError, TypeError):
                        # Si no se puede transformar, evitamos romper el flujo y omitimos ROC
                        st.info(
                            "No se pudo preparar el target para ROC AUC en este modelo.")
                        continue

                try:
                    _plot_roc_curve(y_test_roc, pipeline, X_test)
                except (ValueError, TypeError) as exc:
                    st.info(
                        f"No se pudo generar la curva ROC para {model_name}: {exc}")
            else:
                _plot_residuals(y_test, y_pred)

    _render_model_details(project.trained_models,
                          project.metrics, project, learn)

    if project.trained_models:
        model_options = list(project.trained_models.keys())
        selected_model = st.selectbox(
            "Selecciona un modelo para guardar",
            options=model_options,
        )
        if learn:
            with st.expander("üíæ ¬øQu√© significa guardar un modelo?"):
                st.markdown(
                    "Guardar un modelo significa conservar el modelo ya entrenado para usarlo despu√©s sin volver a entrenar.\n\n"
                    "Por ejemplo, pod√©s cargarlo m√°s adelante para hacer predicciones con nuevos datos."
                )
    cols = st.columns(3)
    if project.trained_models:
        with cols[1]:
            if st.button("Guardar modelo seleccionado"):
                model = project.trained_models[selected_model]
                filename = f"{selected_model}_{project.problem_type}"
                try:
                    path = save_model(model, filename)
                except (OSError, ValueError, TypeError) as exc:
                    st.error(f"No se pudo guardar el modelo: {exc}")
                else:
                    st.success(f"Modelo guardado en {path}")
    with cols[2]:
        if st.button("Guardar configuraci√≥n del proyecto"):
            try:
                path = save_project_config(project.to_dict())
            except (OSError, ValueError, TypeError) as exc:
                st.error(f"No se pudo guardar la configuraci√≥n: {exc}")
            else:
                st.success(f"Configuraci√≥n guardada en {path}")
    with cols[0]:
        if st.button("¬°PRUEBA TU MODELO!"):
            st.switch_page("pages/7-Predicciones.py")


if __name__ == "__main__":
    main()
